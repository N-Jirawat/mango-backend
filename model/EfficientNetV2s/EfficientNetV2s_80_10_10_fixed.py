import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import cv2
import time
from datetime import datetime, timedelta

from sklearn.metrics import (
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    precision_recall_fscore_support, roc_curve, auc, roc_auc_score
)
from sklearn.utils import class_weight
from tensorflow.keras import layers, models, mixed_precision
from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2S, preprocess_input
from tensorflow.keras.callbacks import ReduceLROnPlateau

# ----------------------------
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GPU Growth
# ----------------------------
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

# ----------------------------
# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# ----------------------------
script_start_time = time.time()
training_start_datetime = datetime.now()

# ----------------------------
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
# ----------------------------
report_dir = r'C:\Users\Asus\OneDrive\‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\Report_Model\EfficientNetV2S\EfficientNetV2s_ 80_R10'
os.makedirs(report_dir, exist_ok=True)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
report_file = os.path.join(report_dir, f'training_report_{timestamp}.txt')

def log_and_print(message):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    print(message)
    with open(report_file, 'a', encoding='utf-8') as f:
        f.write(message + '\n')

def format_time(seconds):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö HH:MM:SS"""
    return str(timedelta(seconds=int(seconds)))

def get_elapsed_time(start_time):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"""
    return time.time() - start_time

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
log_and_print("="*80)
log_and_print("üöÄ EfficientNetV2S Training Report")
log_and_print(f"üìÖ Start Time: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
log_and_print("="*80)

# ----------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠: CLAHE + EfficientNet Preprocessing
# ----------------------------
def apply_clahe_np(image):
    image = np.array(image)
    img_uint8 = image.astype(np.uint8)
    lab = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    cl = clahe.apply(l)
    limg = cv2.merge((cl, a, b))
    final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)
    return final

def apply_clahe_tf(image):
    def _clahe(image):
        image = tf.py_function(func=apply_clahe_np, inp=[image], Tout=tf.uint8)
        image.set_shape([224, 224, 3])
        return image
    image = tf.map_fn(_clahe, image, fn_output_signature=tf.uint8)
    return image

def preprocess_with_clahe(image, label):
    image = apply_clahe_tf(image)
    image = tf.cast(image, tf.float32)
    image = preprocess_input(image)
    return image, label

# ----------------------------
# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Pipeline)
# ----------------------------
data_prep_start = time.time()

AUTOTUNE = tf.data.AUTOTUNE
IMG_SIZE = (224, 224)
BATCH_SIZE = 16

train_dir = r'C:\Mango-Disease-80_10_10\train'
val_dir = r'C:\Mango-Disease-80_10_10\val'
test_dir = r'C:\Mango-Disease-80_10_10\test'

# ‡∏ï‡∏±‡πâ‡∏á seed ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡∏•‡∏≠‡∏î
tf.random.set_seed(42)
np.random.seed(42)

log_and_print("\nüìä Loading Dataset...")

train_raw = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=True,
    seed=42
)
val_raw = tf.keras.utils.image_dataset_from_directory(
    val_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=True,
    seed=42
)
test_raw = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=False
)

class_names = train_raw.class_names
num_classes = len(class_names)

log_and_print(f"‚úÖ Dataset loaded successfully!")
log_and_print(f"üìã Number of classes: {num_classes}")
log_and_print(f"üìã Class names: {', '.join(class_names)}")

# Cache ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡∏•‡∏≠‡∏î
train_raw = train_raw.cache()
val_raw = val_raw.cache()
test_raw = test_raw.cache()

# Map CLAHE + EfficientNet preprocess ‚Üí ‡πÅ‡∏•‡πâ‡∏ß prefetch
train_ds = train_raw.map(preprocess_with_clahe, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)
val_ds = val_raw.map(preprocess_with_clahe, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)
test_ds = test_raw.map(preprocess_with_clahe, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)

data_prep_time = time.time() - data_prep_start
log_and_print(f"‚è±Ô∏è Data preparation time: {format_time(data_prep_time)}")

# ----------------------------
# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Class Weights
# ----------------------------
class_weight_start = time.time()
log_and_print("\n‚öñÔ∏è Calculating class weights...")

y_train = np.concatenate([y.numpy() for _, y in train_raw], axis=0)
y_labels = np.argmax(y_train, axis=1)
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_labels),
    y=y_labels
)
class_weights = dict(enumerate(class_weights))

class_weight_time = time.time() - class_weight_start
log_and_print("‚úÖ Class weights calculated:")
for i, weight in class_weights.items():
    log_and_print(f"   {class_names[i]}: {weight:.4f}")
log_and_print(f"‚è±Ô∏è Class weight calculation time: {format_time(class_weight_time)}")

# ----------------------------
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏û‡∏£‡πâ‡∏≠‡∏° mixed precision
# ----------------------------
model_creation_start = time.time()
mixed_precision.set_global_policy('mixed_float16')

def create_model():
    base_model = EfficientNetV2S(
        include_top=False,
        input_shape=IMG_SIZE + (3,),
        weights='imagenet'
    )
    base_model.trainable = False  # freeze base model ‡πÉ‡∏ô Phase 1
    
    inputs = tf.keras.Input(shape=IMG_SIZE + (3,))
    x = base_model(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)
    
    model = models.Model(inputs, outputs)
    return model, base_model

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å
model, base_model = create_model()

model_creation_time = time.time() - model_creation_start
log_and_print(f"\nüèóÔ∏è Model created successfully!")
log_and_print(f"üìä Total parameters: {model.count_params():,}")
log_and_print(f"‚è±Ô∏è Model creation time: {format_time(model_creation_time)}")

# ----------------------------
# Phase 1: Pretrain (Freeze Base Model) - 20 epochs
# ----------------------------
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=2)]
)

phase1_start = time.time()
log_and_print("\nüöÄ Starting Phase 1: Pretrain (Freeze Base Model) - 20 epochs")
log_and_print(f"‚è∞ Phase 1 start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

history1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    class_weight=class_weights,
    callbacks=[reduce_lr],
    verbose=1
)

phase1_time = time.time() - phase1_start
log_and_print("‚úÖ Phase 1 completed!")
log_and_print(f"‚è±Ô∏è Phase 1 training time: {format_time(phase1_time)}")
log_and_print(f"‚è∞ Phase 1 end time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# ----------------------------
# Phase 2: Fine-tune (Unfreeze Base Model) - 30 epochs
# ----------------------------
base_model.trainable = True

reduce_lr_phase2 = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=8,
    min_lr=1e-8,
    verbose=1
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=2)]
)

phase2_start = time.time()
log_and_print("\nüöÄ Starting Phase 2: Fine-tune (Unfreeze Base Model) - 30 epochs")
log_and_print(f"‚è∞ Phase 2 start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

history2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50,
    initial_epoch=20,
    class_weight=class_weights,
    callbacks=[reduce_lr_phase2],
    verbose=1
)

phase2_time = time.time() - phase2_start
total_training_time = phase1_time + phase2_time
log_and_print("‚úÖ Phase 2 completed!")
log_and_print(f"‚è±Ô∏è Phase 2 training time: {format_time(phase2_time)}")
log_and_print(f"‚è∞ Phase 2 end time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
log_and_print(f"‚è±Ô∏è Total training time: {format_time(total_training_time)}")

# ----------------------------
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
# ----------------------------
model_save_start = time.time()
final_model_path = r'C:\Users\Asus\mango-app\backend\models\EfficienNetV2s\model_efficientnetv2s_80_10_10_R10.keras'
model.save(final_model_path)
model_save_time = time.time() - model_save_start

log_and_print(f"\n‚úÖ Final model saved: {final_model_path}")
log_and_print(f"‚è±Ô∏è Model save time: {format_time(model_save_time)}")

# ----------------------------
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü Training History
# ----------------------------
def plot_and_save_history(h1, h2, save_dir, time_info):
    plot_start = time.time()
    
    def safe_get(history_obj, metric):
        if history_obj is None:
            return []
        return history_obj.history.get(metric, [])

    def combine(metric):
        return safe_get(h1, metric) + safe_get(h2, metric)

    if not (combine('accuracy') or combine('val_accuracy') or combine('loss') or combine('val_loss')):
        log_and_print("‚ö†Ô∏è No history data available for plotting")
        return {}

    epochs = range(1, len(combine('loss')) + 1)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
    plt.figure(figsize=(20, 12))
    
    # ‡∏Å‡∏£‡∏≤‡∏ü Accuracy
    plt.subplot(2, 3, 1)
    plt.plot(epochs, combine('accuracy'), 'b-', label='Train Accuracy', linewidth=2, marker='o', markersize=3)
    plt.plot(epochs, combine('val_accuracy'), 'r-', label='Val Accuracy', linewidth=2, marker='s', markersize=3)
    plt.axvline(x=20, color='green', linestyle='--', alpha=0.7, linewidth=2, label='Phase 1 ‚Üí Phase 2')
    plt.title('Model Accuracy', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # ‡∏Å‡∏£‡∏≤‡∏ü Loss
    plt.subplot(2, 3, 2)
    plt.plot(epochs, combine('loss'), 'b-', label='Train Loss', linewidth=2, marker='o', markersize=3)
    plt.plot(epochs, combine('val_loss'), 'r-', label='Val Loss', linewidth=2, marker='s', markersize=3)
    plt.axvline(x=20, color='green', linestyle='--', alpha=0.7, linewidth=2, label='Phase 1 ‚Üí Phase 2')
    plt.title('Model Loss', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # ‡∏Å‡∏£‡∏≤‡∏ü Top-2 Accuracy
    plt.subplot(2, 3, 3)
    plt.plot(epochs, combine('top_k_categorical_accuracy'), 'b-', label='Train Top-2 Acc', linewidth=2, marker='o', markersize=3)
    plt.plot(epochs, combine('val_top_k_categorical_accuracy'), 'r-', label='Val Top-2 Acc', linewidth=2, marker='s', markersize=3)
    plt.axvline(x=20, color='green', linestyle='--', alpha=0.7, linewidth=2, label='Phase 1 ‚Üí Phase 2')
    plt.title('Top-2 Accuracy', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Top-2 Accuracy', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # ‡∏Å‡∏£‡∏≤‡∏é Learning Rate (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    plt.subplot(2, 3, 4)
    if combine('lr'):
        plt.plot(epochs, combine('lr'), 'orange', linewidth=2, marker='d', markersize=3)
        plt.axvline(x=20, color='green', linestyle='--', alpha=0.7, linewidth=2, label='Phase 1 ‚Üí Phase 2')
        plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Learning Rate', fontsize=12)
        plt.yscale('log')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
    else:
        plt.text(0.5, 0.5, 'Learning Rate\nHistory\nNot Available', 
                ha='center', va='center', transform=plt.gca().transAxes,
                fontsize=14, bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray"))
        plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ß‡∏•‡∏≤
    plt.subplot(2, 3, 5)
    final_train_acc = combine('accuracy')[-1] if combine('accuracy') else 0
    final_val_acc = combine('val_accuracy')[-1] if combine('val_accuracy') else 0
    final_train_loss = combine('loss')[-1] if combine('loss') else 0
    final_val_loss = combine('val_loss')[-1] if combine('val_loss') else 0
    final_top2_acc = combine('val_top_k_categorical_accuracy')[-1] if combine('val_top_k_categorical_accuracy') else 0
    
    summary_text = f"""Final Training Results:
    
Train Accuracy: {final_train_acc:.4f}
Val Accuracy: {final_val_acc:.4f}
Train Loss: {final_train_loss:.4f}
Val Loss: {final_val_loss:.4f}
Val Top-2 Acc: {final_top2_acc:.4f}

Training Time:
Phase 1: {format_time(time_info['phase1_time'])}
Phase 2: {format_time(time_info['phase2_time'])}
Total: {format_time(time_info['total_training_time'])}
"""
    
    plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes, fontsize=11,
            verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    plt.axis('off')
    plt.title('Training Summary', fontsize=14, fontweight='bold')
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á validation accuracy
    plt.subplot(2, 3, 6)
    if combine('val_accuracy'):
        max_val_acc = max(combine('val_accuracy'))
        max_val_acc_epoch = combine('val_accuracy').index(max_val_acc) + 1
        
        performance_text = f"""Best Performance:
        
Max Val Accuracy: {max_val_acc:.4f}
At Epoch: {max_val_acc_epoch}

Model Configuration:
‚Ä¢ EfficientNetV2S (Pretrained)
‚Ä¢ Image Size: 224x224
‚Ä¢ Batch Size: 16
‚Ä¢ Mixed Precision: FP16
‚Ä¢ CLAHE Enhancement
‚Ä¢ Class Weights: Balanced

Time Efficiency:
‚Ä¢ Avg Time/Epoch: {format_time(time_info['total_training_time']/50)}
‚Ä¢ Total Training: {format_time(time_info['total_training_time'])}
"""
        
        plt.text(0.1, 0.9, performance_text, transform=plt.gca().transAxes, fontsize=11,
                verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.8))
    
    plt.axis('off')
    plt.title('Best Performance', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü
    graph_path = os.path.join(save_dir, f'training_history_{timestamp}.png')
    plt.savefig(graph_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    plot_time = time.time() - plot_start
    log_and_print(f"‚úÖ Training history graph saved: {graph_path}")
    log_and_print(f"‚è±Ô∏è Plot generation time: {format_time(plot_time)}")
    
    return {
        'final_train_acc': final_train_acc,
        'final_val_acc': final_val_acc,
        'final_train_loss': final_train_loss,
        'final_val_loss': final_val_loss,
        'final_top2_acc': final_top2_acc,
        'max_val_acc': max_val_acc if combine('val_accuracy') else 0,
        'max_val_acc_epoch': max_val_acc_epoch if combine('val_accuracy') else 0
    }

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü
time_info = {
    'phase1_time': phase1_time,
    'phase2_time': phase2_time,
    'total_training_time': total_training_time
}
training_summary = plot_and_save_history(history1, history2, report_dir, time_info)

# ----------------------------
# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô Test Set
# ----------------------------
log_and_print("\nüîç Evaluating model on Test Set...")

y_true = []
y_pred = []
y_pred_proba = []

inference_start = time.time()
for batch_idx, (images, labels) in enumerate(test_ds):
    batch_start = time.time()
    preds = model.predict(images, verbose=0)
    batch_time = time.time() - batch_start
    
    y_pred_proba.extend(preds)
    y_pred.extend(np.argmax(preds, axis=1))
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    
    if batch_idx == 0:  # Log timing for first batch
        log_and_print(f"   First batch ({len(images)} samples) inference time: {batch_time:.4f} seconds")

total_inference_time = time.time() - inference_start

y_true = np.array(y_true)
y_pred = np.array(y_pred)
y_pred_proba = np.array(y_pred_proba)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Classification Report
evaluation_start = time.time()
log_and_print("\n" + "="*80)
log_and_print("üìä TEST SET EVALUATION RESULTS")
log_and_print("="*80)

report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
report_str = classification_report(y_true, y_pred, target_names=class_names)
log_and_print(report_str)

test_accuracy = np.mean(y_true == y_pred)
log_and_print(f"\nüèÜ Test Accuracy: {test_accuracy:.4f}")
log_and_print(f"‚è±Ô∏è Total Inference Time: {format_time(total_inference_time)}")
log_and_print(f"‚ö° Average Time per Sample: {total_inference_time/len(y_true):.6f} seconds")
log_and_print(f"üî¢ Total Test Samples: {len(y_true)}")
log_and_print(f"üìä Inference Speed: {len(y_true)/total_inference_time:.2f} samples/second")

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Top-2 Accuracy
top2_predictions = np.argsort(y_pred_proba, axis=1)[:, -2:]
top2_accuracy = np.mean([y_true[i] in top2_predictions[i] for i in range(len(y_true))])
log_and_print(f"üéØ Top-2 Test Accuracy: {top2_accuracy:.4f}")

evaluation_time = time.time() - evaluation_start
log_and_print(f"‚è±Ô∏è Evaluation computation time: {format_time(evaluation_time)}")

# ----------------------------
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Confusion Matrix
# ----------------------------
cm_start = time.time()
plt.figure(figsize=(12, 10))
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Confusion Matrix
cm_path = os.path.join(report_dir, f'confusion_matrix_{timestamp}.png')
plt.savefig(cm_path, dpi=300, bbox_inches='tight', facecolor='white')
plt.show()

cm_time = time.time() - cm_start
log_and_print(f"‚úÖ Confusion matrix saved: {cm_path}")
log_and_print(f"‚è±Ô∏è Confusion matrix generation time: {format_time(cm_time)}")

# ----------------------------
# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# ----------------------------
total_script_time = time.time() - script_start_time
script_end_datetime = datetime.now()

# ----------------------------
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
# ----------------------------
log_and_print("\n" + "="*80)
log_and_print("üìã FINAL TRAINING SUMMARY")
log_and_print("="*80)
log_and_print(f"üèóÔ∏è Model Architecture: EfficientNetV2S")
log_and_print(f"üìä Total Parameters: {model.count_params():,}")
log_and_print(f"üî¢ Number of Classes: {num_classes}")
log_and_print(f"üìÅ Classes: {', '.join(class_names)}")
log_and_print(f"üñºÔ∏è Image Size: {IMG_SIZE}")
log_and_print(f"üì¶ Batch Size: {BATCH_SIZE}")
log_and_print(f"üîÑ Total Epochs: 50 (Phase 1: 20, Phase 2: 30)")
log_and_print(f"üéØ Fixed Seed: 42")
log_and_print(f"‚öñÔ∏è Class Weights: Balanced")
log_and_print(f"üîß Mixed Precision: FP16")
log_and_print(f"üñºÔ∏è Image Enhancement: CLAHE")

log_and_print(f"\n‚è±Ô∏è Detailed Timing Information:")
log_and_print(f"   Data Preparation: {format_time(data_prep_time)}")
log_and_print(f"   Class Weight Calculation: {format_time(class_weight_time)}")
log_and_print(f"   Model Creation: {format_time(model_creation_time)}")
log_and_print(f"   Phase 1 Training: {format_time(phase1_time)}")
log_and_print(f"   Phase 2 Training: {format_time(phase2_time)}")
log_and_print(f"   Total Training: {format_time(total_training_time)}")
log_and_print(f"   Model Saving: {format_time(model_save_time)}")
log_and_print(f"   Test Inference: {format_time(total_inference_time)}")
log_and_print(f"   Evaluation: {format_time(evaluation_time)}")
log_and_print(f"   Confusion Matrix: {format_time(cm_time)}")
log_and_print(f"   Total Script Runtime: {format_time(total_script_time)}")

log_and_print(f"\nüìà Training Results:")
log_and_print(f"   Final Train Accuracy: {training_summary.get('final_train_acc', 0):.4f}")
log_and_print(f"   Final Val Accuracy: {training_summary.get('final_val_acc', 0):.4f}")
log_and_print(f"   Final Train Loss: {training_summary.get('final_train_loss', 0):.4f}")
log_and_print(f"   Final Val Loss: {training_summary.get('final_val_loss', 0):.4f}")
log_and_print(f"   Final Val Top-2 Accuracy: {training_summary.get('final_top2_acc', 0):.4f}")
log_and_print(f"   Best Val Accuracy: {training_summary.get('max_val_acc', 0):.4f} (Epoch {training_summary.get('max_val_acc_epoch', 0)})")

log_and_print(f"\nüéØ Test Results:")
log_and_print(f"   Test Accuracy: {test_accuracy:.4f}")
log_and_print(f"   Test Top-2 Accuracy: {top2_accuracy:.4f}")
log_and_print(f"   Inference Speed: {len(y_true)/total_inference_time:.2f} samples/second")
log_and_print(f"   Avg Time per Sample: {total_inference_time/len(y_true):.6f} seconds")

log_and_print(f"\n‚è∞ Time Summary:")
log_and_print(f"   Start Time: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
log_and_print(f"   End Time: {script_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
log_and_print(f"   Total Duration: {format_time(total_script_time)}")
log_and_print(f"   Training Efficiency: {50/total_training_time*3600:.2f} epochs/hour")
log_and_print(f"   Average Epoch Time: {format_time(total_training_time/50)}")

log_and_print(f"\nüìÅ Output Files:")
log_and_print(f"   Final Model: {final_model_path}")
log_and_print(f"   Training Report: {report_file}")
log_and_print(f"   Training Graph: {os.path.join(report_dir, f'training_history_{timestamp}.png')}")
log_and_print(f"   Confusion Matrix: {cm_path}")

log_and_print(f"\nüìÖ End Time: {script_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
log_and_print("="*80)
log_and_print("‚úÖ Training completed successfully!")
log_and_print(f"üéâ Total execution time: {format_time(total_script_time)}")
log_and_print("="*80)

# ----------------------------
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå
# ----------------------------
time_report_file = os.path.join(report_dir, f'time_analysis_{timestamp}.txt')
with open(time_report_file, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("‚è±Ô∏è DETAILED TIME ANALYSIS REPORT\n")
    f.write("="*80 + "\n")
    f.write(f"Training Start: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Training End: {script_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Total Duration: {format_time(total_script_time)}\n\n")
    
    f.write("üìä Time Breakdown:\n")
    f.write(f"‚îú‚îÄ‚îÄ Data Preparation: {format_time(data_prep_time)} ({data_prep_time/total_script_time*100:.1f}%)\n")
    f.write(f"‚îú‚îÄ‚îÄ Class Weight Calculation: {format_time(class_weight_time)} ({class_weight_time/total_script_time*100:.1f}%)\n")
    f.write(f"‚îú‚îÄ‚îÄ Model Creation: {format_time(model_creation_time)} ({model_creation_time/total_script_time*100:.1f}%)\n")
    f.write(f"‚îú‚îÄ‚îÄ Training (Total): {format_time(total_training_time)} ({total_training_time/total_script_time*100:.1f}%)\n")
    f.write(f"‚îÇ   ‚îú‚îÄ‚îÄ Phase 1 (20 epochs): {format_time(phase1_time)} ({phase1_time/total_training_time*100:.1f}% of training)\n")
    f.write(f"‚îÇ   ‚îî‚îÄ‚îÄ Phase 2 (30 epochs): {format_time(phase2_time)} ({phase2_time/total_training_time*100:.1f}% of training)\n")
    f.write(f"‚îú‚îÄ‚îÄ Model Saving: {format_time(model_save_time)} ({model_save_time/total_script_time*100:.1f}%)\n")
    f.write(f"‚îú‚îÄ‚îÄ Test Inference: {format_time(total_inference_time)} ({total_inference_time/total_script_time*100:.1f}%)\n")
    f.write(f"‚îú‚îÄ‚îÄ Evaluation: {format_time(evaluation_time)} ({evaluation_time/total_script_time*100:.1f}%)\n")
    f.write(f"‚îî‚îÄ‚îÄ Visualization: {format_time(cm_time)} ({cm_time/total_script_time*100:.1f}%)\n\n")
    
    f.write("üèÉ Performance Metrics:\n")
    f.write(f"‚Ä¢ Training Speed: {50/total_training_time*3600:.2f} epochs/hour\n")
    f.write(f"‚Ä¢ Average Epoch Time: {format_time(total_training_time/50)}\n")
    f.write(f"‚Ä¢ Phase 1 Avg Epoch: {format_time(phase1_time/20)}\n")
    f.write(f"‚Ä¢ Phase 2 Avg Epoch: {format_time(phase2_time/30)}\n")
    f.write(f"‚Ä¢ Inference Speed: {len(y_true)/total_inference_time:.2f} samples/second\n")
    f.write(f"‚Ä¢ Time per Sample: {total_inference_time/len(y_true)*1000:.2f} ms\n\n")
    
    f.write("üéØ Efficiency Analysis:\n")
    f.write(f"‚Ä¢ Training Efficiency: {(phase1_time + phase2_time)/total_script_time*100:.1f}% of total time\n")
    f.write(f"‚Ä¢ Setup Overhead: {(data_prep_time + class_weight_time + model_creation_time)/total_script_time*100:.1f}% of total time\n")
    f.write(f"‚Ä¢ Evaluation Overhead: {(total_inference_time + evaluation_time + cm_time)/total_script_time*100:.1f}% of total time\n")
    
    if phase2_time > 0 and phase1_time > 0:
        phase_ratio = phase2_time / phase1_time
        f.write(f"‚Ä¢ Phase 2 vs Phase 1 Time Ratio: {phase_ratio:.2f}x\n")
        f.write(f"‚Ä¢ Time per Epoch Comparison: Phase 2 takes {(phase2_time/30)/(phase1_time/20):.2f}x longer per epoch\n")

log_and_print(f"\nüìä Detailed time analysis saved: {time_report_file}")

# ----------------------------
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
# ----------------------------
import json

timing_data = {
    "experiment_info": {
        "model": "EfficientNetV2S",
        "total_epochs": 50,
        "phase1_epochs": 20,
        "phase2_epochs": 30,
        "batch_size": BATCH_SIZE,
        "image_size": IMG_SIZE,
        "num_classes": num_classes,
        "total_parameters": int(model.count_params()),
        "start_datetime": training_start_datetime.isoformat(),
        "end_datetime": script_end_datetime.isoformat()
    },
    "timing_breakdown": {
        "data_preparation_seconds": float(data_prep_time),
        "class_weight_calculation_seconds": float(class_weight_time),
        "model_creation_seconds": float(model_creation_time),
        "phase1_training_seconds": float(phase1_time),
        "phase2_training_seconds": float(phase2_time),
        "total_training_seconds": float(total_training_time),
        "model_saving_seconds": float(model_save_time),
        "inference_seconds": float(total_inference_time),
        "evaluation_seconds": float(evaluation_time),
        "visualization_seconds": float(cm_time),
        "total_script_seconds": float(total_script_time)
    },
    "performance_metrics": {
        "epochs_per_hour": float(50/total_training_time*3600),
        "average_epoch_seconds": float(total_training_time/50),
        "phase1_avg_epoch_seconds": float(phase1_time/20),
        "phase2_avg_epoch_seconds": float(phase2_time/30),
        "inference_samples_per_second": float(len(y_true)/total_inference_time),
        "milliseconds_per_sample": float(total_inference_time/len(y_true)*1000),
        "test_samples": len(y_true)
    },
    "training_results": {
        "final_train_accuracy": float(training_summary.get('final_train_acc', 0)),
        "final_val_accuracy": float(training_summary.get('final_val_acc', 0)),
        "best_val_accuracy": float(training_summary.get('max_val_acc', 0)),
        "best_val_accuracy_epoch": int(training_summary.get('max_val_acc_epoch', 0)),
        "test_accuracy": float(test_accuracy),
        "test_top2_accuracy": float(top2_accuracy)
    }
}

json_report_file = os.path.join(report_dir, f'timing_data_{timestamp}.json')
with open(json_report_file, 'w', encoding='utf-8') as f:
    json.dump(timing_data, f, indent=2, ensure_ascii=False)

log_and_print(f"üìà Timing data (JSON) saved: {json_report_file}")
log_and_print("\nüéä All reports and analyses completed successfully!")